{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcdb586-95b6-4c71-ae28-0e25d79a14bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DDPM \n",
    "\n",
    "Denoising Diffusion Probabilistic Model with PyTorch implementation\n",
    "\n",
    "### References\n",
    "- https://github.com/hojonathanho/diffusion\n",
    "- https://github.com/lucidrains/denoising-diffusion-pytorch\n",
    "- https://github.com/openai/improved-diffusion/\n",
    "- https://arxiv.org/abs/2006.11239\n",
    "- https://huggingface.co/blog/annotated-diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4bcffb-c456-41ae-934b-3499fb7969c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b17bcf5-8277-44cf-adff-e56b85f10082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sinusoidal position embedding to encode time step (https://arxiv.org/abs/1706.03762)   \n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28fd07cd-ef76-4977-94df-03f854688cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define TimestepEmbedSequential to support `time_emb` as extra input\n",
    "class TimestepBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Any module where forward() takes timestep embeddings as a second argument.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "# use GN for norm layer\n",
    "def norm_layer(channels):\n",
    "    return nn.GroupNorm(32, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0097b55e-7817-418b-88ba-91d2603ed2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block\n",
    "class ResidualBlock(TimestepBlock):\n",
    "    def __init__(self, in_channels, out_channels, time_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            norm_layer(in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # pojection for time step embedding\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            norm_layer(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        `x` has shape `[batch_size, in_dim, height, width]`\n",
    "        `t` has shape `[batch_size, time_dim]`\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        # Add time step embeddings\n",
    "        h += self.time_emb(t)[:, :, None, None]\n",
    "        h = self.conv2(h)\n",
    "        return h + self.shortcut(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ad5b1e4-4ec8-4d73-ac9d-891fbc3bfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention block with shortcut\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert channels % num_heads == 0\n",
    "        \n",
    "        self.norm = norm_layer(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        q, k, v = qkv.reshape(B*self.num_heads, -1, H*W).chunk(3, dim=1)\n",
    "        scale = 1. / math.sqrt(math.sqrt(C // self.num_heads))\n",
    "        attn = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        h = torch.einsum(\"bts,bcs->bct\", attn, v)\n",
    "        h = h.reshape(B, -1, H, W)\n",
    "        h = self.proj(h)\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c4d900-190d-45b5-be8f-4ee1ef5f0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# downsample\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels, use_conv):\n",
    "        super().__init__()\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.op = nn.AvgPool2d(stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "085a1f88-4257-4db9-88cb-d8305a6a4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full UNet model with attention and timestep embedding\n",
    "class UNetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        model_channels=128,\n",
    "        out_channels=3,\n",
    "        num_res_blocks=2,\n",
    "        attention_resolutions=(8, 16),\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 2, 2),\n",
    "        conv_resample=True,\n",
    "        num_heads=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # time embedding\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "        \n",
    "        # down blocks\n",
    "        self.down_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1))\n",
    "        ])\n",
    "        down_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResidualBlock(ch, mult * model_channels, time_embed_dim, dropout)\n",
    "                ]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                self.down_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                down_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1: # don't use downsample for the last stage\n",
    "                self.down_blocks.append(TimestepEmbedSequential(Downsample(ch, conv_resample)))\n",
    "                down_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "        \n",
    "        # middle block\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResidualBlock(ch, ch, time_embed_dim, dropout),\n",
    "            AttentionBlock(ch, num_heads=num_heads),\n",
    "            ResidualBlock(ch, ch, time_embed_dim, dropout)\n",
    "        )\n",
    "        \n",
    "        # up blocks\n",
    "        self.up_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                layers = [\n",
    "                    ResidualBlock(\n",
    "                        ch + down_block_chans.pop(),\n",
    "                        model_channels * mult,\n",
    "                        time_embed_dim,\n",
    "                        dropout\n",
    "                    )\n",
    "                ]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                if level and i == num_res_blocks:\n",
    "                    layers.append(Upsample(ch, conv_resample))\n",
    "                    ds //= 2\n",
    "                self.up_blocks.append(TimestepEmbedSequential(*layers))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            norm_layer(ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(model_channels, out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x H x W] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        # time step embedding\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "        \n",
    "        # down stage\n",
    "        h = x\n",
    "        for module in self.down_blocks:\n",
    "            h = module(h, emb)\n",
    "            hs.append(h)\n",
    "        # middle stage\n",
    "        h = self.middle_block(h, emb)\n",
    "        # up stage\n",
    "        for module in self.up_blocks:\n",
    "            cat_in = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = module(cat_in, emb)\n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f9ff7e-5be2-4103-b5bc-14e94ae98589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta schedule\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a882a5-7358-4ead-aa04-de4ada2e0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        timesteps=1000,\n",
    "        beta_schedule='linear'\n",
    "    ):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "        self.betas = betas\n",
    "            \n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "        \n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "        \n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning\n",
    "        # of the diffusion chain\n",
    "        self.posterior_log_variance_clipped = torch.log(self.posterior_variance.clamp(min =1e-20))\n",
    "        \n",
    "        self.posterior_mean_coef1 = (\n",
    "            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * torch.sqrt(self.alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    # get the param of given timestep t\n",
    "    def _extract(self, a, t, x_shape):\n",
    "        batch_size = t.shape[0]\n",
    "        out = a.to(t.device).gather(0, t).float()\n",
    "        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "        return out\n",
    "    \n",
    "    # forward diffusion (using the nice property): q(x_t | x_0)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    # Get the mean and variance of q(x_t | x_0).\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "    \n",
    "    # Compute the mean and variance of the diffusion posterior: q(x_{t-1} | x_t, x_0)\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "    \n",
    "    # compute x_0 from x_t and pred noise: the reverse of `q_sample`\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "    \n",
    "    # compute predicted mean and variance of p(x_{t-1} | x_t)\n",
    "    def p_mean_variance(self, model, x_t, t, clip_denoised=True):\n",
    "        # predict noise using model\n",
    "        pred_noise = model(x_t, t)\n",
    "        # get the predicted x_0: different from the algorithm2 in the paper\n",
    "        x_recon = self.predict_start_from_noise(x_t, t, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x_recon = torch.clamp(x_recon, min=-1., max=1.)\n",
    "        model_mean, posterior_variance, posterior_log_variance = \\\n",
    "                    self.q_posterior_mean_variance(x_recon, x_t, t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance\n",
    "        \n",
    "    # denoise_step: sample x_{t-1} from x_t and pred_noise\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t, t, clip_denoised=True):\n",
    "        # predict mean and variance\n",
    "        model_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t,\n",
    "                                                    clip_denoised=clip_denoised)\n",
    "        noise = torch.randn_like(x_t)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))\n",
    "        # compute x_{t-1}\n",
    "        pred_img = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img\n",
    "    \n",
    "    # denoise: reverse diffusion\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape):\n",
    "        batch_size = shape[0]\n",
    "        device = next(model.parameters()).device\n",
    "        # start from pure noise (for each example in the batch)\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "        for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
    "            img = self.p_sample(model, img, torch.full((batch_size,), i, device=device, dtype=torch.long))\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "    \n",
    "    # sample new images\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=8, channels=3):\n",
    "        return self.p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))\n",
    "    \n",
    "    # compute train losses\n",
    "    def train_losses(self, model, x_start, t):\n",
    "        # generate random noise\n",
    "        noise = torch.randn_like(x_start)\n",
    "        # get x_t\n",
    "        x_noisy = self.q_sample(x_start, t, noise=noise)\n",
    "        predicted_noise = model(x_noisy, t)\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7288bb44-c7a1-4a5e-bc4d-1046e07cc8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chilin/.pyenv/versions/3.8.12/lib/python3.8/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "batch_size = 64\n",
    "timesteps = 1000\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# use MNIST dataset\n",
    "# dataset = datasets.ImageFolder('./mnist', transform=transform)\n",
    "\n",
    "class mnist_dataset(Dataset):\n",
    "    def __init__(self, mnist_paths, transform) -> None:\n",
    "        self.mnist_paths = mnist_paths\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x = cv2.imread(self.mnist_paths[index])\n",
    "        x = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n",
    "        return self.transform(x)\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_paths)\n",
    "\n",
    "\n",
    "files = os.listdir(\"./mnist\")\n",
    "files = [os.path.join(\"/home/chilin/NYCU-Cloud-Computing-and-Big-Data-Analytics/HW3/mnist\", mmnist_file) for mmnist_file in files]\n",
    "dataset = mnist_dataset(files, transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# define model and diffusion\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = UNetModel(\n",
    "    in_channels=1,\n",
    "    model_channels=96,\n",
    "    out_channels=1,\n",
    "    channel_mult=(1, 2, 2),\n",
    "    attention_resolutions=[]\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "gaussian_diffusion = GaussianDiffusion(timesteps=timesteps)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db52846-e9d1-41ae-b85c-cefe1fd91d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Loss: 0.9308072924613953\n",
      "Loss: 0.04100961983203888\n",
      "Loss: 0.033094506710767746\n",
      "Loss: 0.0295766219496727\n",
      "Loss: 0.03497980535030365\n",
      "epoch: 1\n",
      "Loss: 0.02189999632537365\n",
      "Loss: 0.023282010108232498\n",
      "Loss: 0.02885095402598381\n",
      "Loss: 0.0228777676820755\n",
      "Loss: 0.02323233149945736\n",
      "epoch: 2\n",
      "Loss: 0.02512933313846588\n",
      "Loss: 0.021223418414592743\n",
      "Loss: 0.02465861663222313\n",
      "Loss: 0.02698872797191143\n",
      "Loss: 0.03004184551537037\n",
      "epoch: 3\n",
      "Loss: 0.028568772599101067\n",
      "Loss: 0.022440355271100998\n",
      "Loss: 0.01761030964553356\n",
      "Loss: 0.024350056424736977\n",
      "Loss: 0.019119109958410263\n",
      "epoch: 4\n",
      "Loss: 0.02621404640376568\n",
      "Loss: 0.028168749064207077\n",
      "Loss: 0.01941940374672413\n",
      "Loss: 0.02552483044564724\n",
      "Loss: 0.01568935625255108\n",
      "epoch: 5\n",
      "Loss: 0.01742849312722683\n",
      "Loss: 0.026837075129151344\n",
      "Loss: 0.02255789004266262\n",
      "Loss: 0.02448219805955887\n",
      "Loss: 0.02214927040040493\n",
      "epoch: 6\n",
      "Loss: 0.022805582731962204\n",
      "Loss: 0.02537844143807888\n",
      "Loss: 0.020282182842493057\n",
      "Loss: 0.02378750964999199\n",
      "Loss: 0.018455510959029198\n",
      "epoch: 7\n",
      "Loss: 0.024687591940164566\n",
      "Loss: 0.013858855701982975\n",
      "Loss: 0.027142532169818878\n",
      "Loss: 0.02598084881901741\n",
      "Loss: 0.026985013857483864\n",
      "epoch: 8\n",
      "Loss: 0.024767175316810608\n",
      "Loss: 0.03217122331261635\n",
      "Loss: 0.018963485956192017\n",
      "Loss: 0.01647651568055153\n",
      "Loss: 0.028595076873898506\n",
      "epoch: 9\n",
      "Loss: 0.03635317087173462\n",
      "Loss: 0.02354990504682064\n",
      "Loss: 0.019761325791478157\n",
      "Loss: 0.019236141815781593\n",
      "Loss: 0.024954337626695633\n",
      "epoch: 10\n",
      "Loss: 0.023036666214466095\n",
      "Loss: 0.016868798062205315\n",
      "Loss: 0.01858127862215042\n",
      "Loss: 0.020949020981788635\n",
      "Loss: 0.029016509652137756\n",
      "epoch: 11\n",
      "Loss: 0.027181103825569153\n",
      "Loss: 0.015897603705525398\n",
      "Loss: 0.021533221006393433\n",
      "Loss: 0.02078377455472946\n",
      "Loss: 0.023689309135079384\n",
      "epoch: 12\n",
      "Loss: 0.021856514737010002\n",
      "Loss: 0.026663629338145256\n",
      "Loss: 0.025038905441761017\n",
      "Loss: 0.02273397520184517\n",
      "Loss: 0.022190628573298454\n",
      "epoch: 13\n",
      "Loss: 0.023086698725819588\n",
      "Loss: 0.02276678942143917\n",
      "Loss: 0.02848796360194683\n",
      "Loss: 0.01896723173558712\n",
      "Loss: 0.03069096803665161\n",
      "epoch: 14\n",
      "Loss: 0.018182650208473206\n",
      "Loss: 0.01860782690346241\n",
      "Loss: 0.02137577533721924\n",
      "Loss: 0.016097309067845345\n",
      "Loss: 0.02373780868947506\n",
      "epoch: 15\n",
      "Loss: 0.018874552100896835\n",
      "Loss: 0.01588156446814537\n",
      "Loss: 0.017554767429828644\n",
      "Loss: 0.019505664706230164\n",
      "Loss: 0.027549469843506813\n",
      "epoch: 16\n",
      "Loss: 0.016514673829078674\n",
      "Loss: 0.02363264188170433\n",
      "Loss: 0.015576958656311035\n",
      "Loss: 0.015247557312250137\n",
      "Loss: 0.02050703763961792\n",
      "epoch: 17\n",
      "Loss: 0.01883545145392418\n",
      "Loss: 0.021368814632296562\n",
      "Loss: 0.01899430714547634\n",
      "Loss: 0.022073090076446533\n",
      "Loss: 0.019671179354190826\n",
      "epoch: 18\n",
      "Loss: 0.023698190227150917\n",
      "Loss: 0.026680955663323402\n",
      "Loss: 0.020755967125296593\n",
      "Loss: 0.020164012908935547\n",
      "Loss: 0.023856138810515404\n",
      "epoch: 19\n",
      "Loss: 0.02758324332535267\n",
      "Loss: 0.022279685363173485\n",
      "Loss: 0.02631899155676365\n",
      "Loss: 0.02043897658586502\n",
      "Loss: 0.021000972017645836\n",
      "epoch: 20\n",
      "Loss: 0.024933265522122383\n",
      "Loss: 0.02318679913878441\n",
      "Loss: 0.022929644212126732\n",
      "Loss: 0.020767532289028168\n",
      "Loss: 0.01775519922375679\n",
      "epoch: 21\n",
      "Loss: 0.02244093455374241\n",
      "Loss: 0.01785476878285408\n",
      "Loss: 0.022608721628785133\n",
      "Loss: 0.02128811739385128\n",
      "Loss: 0.021441273391246796\n",
      "epoch: 22\n",
      "Loss: 0.019440604373812675\n",
      "Loss: 0.018822789192199707\n",
      "Loss: 0.017223304137587547\n",
      "Loss: 0.01957712136209011\n",
      "Loss: 0.02096492610871792\n",
      "epoch: 23\n",
      "Loss: 0.020453298464417458\n",
      "Loss: 0.01572045497596264\n",
      "Loss: 0.025761814787983894\n",
      "Loss: 0.019952381029725075\n",
      "Loss: 0.02108731120824814\n",
      "epoch: 24\n",
      "Loss: 0.023652371019124985\n",
      "Loss: 0.01789853349328041\n",
      "Loss: 0.019213642925024033\n",
      "Loss: 0.016695503145456314\n",
      "Loss: 0.027204571291804314\n",
      "epoch: 25\n",
      "Loss: 0.027769288048148155\n",
      "Loss: 0.017468014732003212\n",
      "Loss: 0.025236351415514946\n",
      "Loss: 0.024283427745103836\n",
      "Loss: 0.024849580600857735\n",
      "epoch: 26\n",
      "Loss: 0.019015267491340637\n",
      "Loss: 0.0256021898239851\n",
      "Loss: 0.026060989126563072\n",
      "Loss: 0.02276756800711155\n",
      "Loss: 0.022281669080257416\n",
      "epoch: 27\n",
      "Loss: 0.026296164840459824\n",
      "Loss: 0.018213840201497078\n",
      "Loss: 0.017171179875731468\n",
      "Loss: 0.019577311351895332\n",
      "Loss: 0.017985874786973\n",
      "epoch: 28\n",
      "Loss: 0.02208605967462063\n",
      "Loss: 0.019051048904657364\n",
      "Loss: 0.02096402831375599\n",
      "Loss: 0.018346376717090607\n",
      "Loss: 0.01943432167172432\n",
      "epoch: 29\n",
      "Loss: 0.01879722811281681\n",
      "Loss: 0.018241075798869133\n",
      "Loss: 0.017854776233434677\n",
      "Loss: 0.01849672943353653\n",
      "Loss: 0.019144263118505478\n"
     ]
    }
   ],
   "source": [
    "# # train\n",
    "# epochs = 30\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"epoch: {epoch}\")\n",
    "#     for step, images in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         batch_size = images.shape[0]\n",
    "#         images = images.to(device)\n",
    "        \n",
    "#         # sample t uniformally for every example in the batch\n",
    "#         t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "#         loss = gaussian_diffusion.train_losses(model, images, t)\n",
    "        \n",
    "#         if step % 200 == 0:\n",
    "#             print(\"Loss:\", loss.item())\n",
    "            \n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e095e059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [13:36<00:00,  1.22it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# print(type(generated_images[0][0]))\n",
    "from matplotlib.pyplot import figure\n",
    "import torch\n",
    "\n",
    "plt.ioff()\n",
    "model = torch.load(\"best_model\")\n",
    "cnt = 1\n",
    "for z in range(10):\n",
    "    generated_images = gaussian_diffusion.sample(model, 28, batch_size=1000, channels=1)\n",
    "    imgs = generated_images[-1]\n",
    "    imgs = imgs.reshape(1000, 28, 28)\n",
    "    for j in range(1000):\n",
    "        figure(figsize=(28/1, 28/1), dpi=1)\n",
    "        plt.axis(\"off\")\n",
    "        # plt.imshow(imgs[j], cmap=\"gray\")\n",
    "        plt.imsave(f\"gen/{cnt}.png\", imgs[j], cmap=\"gray\")\n",
    "        # plt.savefig(f\"gen/{cnt}.png\", bbox_inches='tight')\n",
    "        cnt += 1\n",
    "        if cnt>10000:\n",
    "            break\n",
    "    if cnt>10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dccd8d3-68fa-4cea-bc43-0adf0557f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"best_model\")\n",
    "generated_images = gaussian_diffusion.sample(model, 28, batch_size=8, channels=1)\n",
    "\n",
    "# show the denoise steps\n",
    "fig = plt.figure(figsize=(12, 12), constrained_layout=True)\n",
    "gs = fig.add_gridspec(8, 8)\n",
    "plt.ion()\n",
    "for n_row in range(8):\n",
    "    for n_col in range(1, 8):\n",
    "        f_ax = fig.add_subplot(gs[n_col, n_row])\n",
    "        t_idx = int(1000 / 7) * n_col if n_col < 7 else -1\n",
    "        img = generated_images[t_idx][n_row].reshape(28, 28)\n",
    "        f_ax.imshow(img, cmap=\"gray\")\n",
    "        f_ax.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('3.8.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "44883d22f80f18ef889a1dba7be5a0fad8fd72b0a76884ea66d1ab4eda69ec6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
