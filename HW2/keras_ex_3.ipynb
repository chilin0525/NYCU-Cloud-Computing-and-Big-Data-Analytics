{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 07:22:31.687217: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-06 07:22:31.750064: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-06 07:22:31.767017: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-06 07:22:32.095628: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-11-06 07:22:32.095672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2022-11-06 07:22:32.095675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-06 07:22:32.529850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-06 07:22:33.246553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.264359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.264470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.628811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.628933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.629018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.629074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10226 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset hyperparameters\n",
    "unlabeled_dataset_size = 7294\n",
    "labeled_dataset_size = 400\n",
    "image_size = 96\n",
    "image_channels = 3\n",
    "\n",
    "# Algorithm hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 256  # Corresponds to 200 steps per epoch\n",
    "width = 128\n",
    "temperature = 0.1\n",
    "# Stronger augmentations for contrastive, weaker ones for supervised training\n",
    "contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2}\n",
    "classification_augmentation = {\"min_area\": 0.75, \"brightness\": 0.3, \"jitter\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7294\n",
      "400\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "unlabel_path = \"./unlabeled\"\n",
    "unlabel_files = os.listdir(unlabel_path)\n",
    "unlabel_files = [os.path.join(unlabel_path, i) for i in unlabel_files]\n",
    "\n",
    "label_path = \"./test\"\n",
    "label_fold_files = os.listdir(label_path)\n",
    "label_fold_files = [os.path.join(label_path, i) for i in label_fold_files]\n",
    "label_train_files = list()\n",
    "label_test_files = list()\n",
    "for fold in label_fold_files:\n",
    "    fold_files = os.listdir(fold)\n",
    "    train, test = train_test_split(fold_files, shuffle=True, test_size=0.2)\n",
    "    label_train_files.extend([(os.path.join(fold, i), fold.split('/')[-1]) for i in train])\n",
    "    label_test_files.extend([(os.path.join(fold, i), fold.split('/')[-1]) for i in test])\n",
    "\n",
    "print(len(unlabel_files))\n",
    "print(len(label_train_files))\n",
    "print(len(label_test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./test/3/0417.jpg', '3')\n",
      "('./test/3/0150.jpg', '3')\n"
     ]
    }
   ],
   "source": [
    "print(label_train_files[0])\n",
    "print(label_test_files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_mri(mri_path):\n",
    "    mri_data = cv2.imread(mri_path)\n",
    "    return mri_data\n",
    "\n",
    "def unlabel_data_generator(filename_list, batch_size=1):\n",
    "    idx = 0\n",
    "    while idx<len(filename_list):\n",
    "        pixel_data_list = list()\n",
    "        ground_true_list = list()\n",
    "        for j in range(batch_size):\n",
    "            if idx>=len(filename_list):\n",
    "                break\n",
    "            mri_filename = filename_list[idx]\n",
    "            mri_filename = mri_filename.decode('ascii')\n",
    "            pixel_data = read_mri(mri_filename)\n",
    "            pixel_data_list.append(pixel_data)\n",
    "            idx += 1\n",
    "        yield (np.array(pixel_data_list), np.array([None]*len(pixel_data_list)))\n",
    "\n",
    "\n",
    "def label_data_generator(filename_list, batch_size=1):\n",
    "    idx = 0\n",
    "    while idx<len(filename_list):\n",
    "        mri_data_list = list()\n",
    "        ground_true_list = list()\n",
    "        for j in range(batch_size):\n",
    "            if idx>=len(filename_list):\n",
    "                break\n",
    "            mri_filename, y_true = filename_list[idx]\n",
    "            mri_filename = mri_filename.decode('ascii')\n",
    "            pixel_data = read_mri(mri_filename)\n",
    "            mri_data_list.append(pixel_data)\n",
    "            ground_true_list.append(y_true)\n",
    "            idx += 1\n",
    "        yield (np.array(mri_data_list), np.array(ground_true_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./unlabeled/3362.jpg', './unlabeled/3370.jpg', './unlabeled/5102.jpg', './unlabeled/5279.jpg', './unlabeled/1144.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(unlabel_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 07:22:33.982727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.983067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.983230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.983777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.983957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.984105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.984327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.984810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-06 07:22:33.985047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10226 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# generate Dataset\n",
    "unlabel_dataset = tf.data.Dataset.from_generator(\n",
    "    unlabel_data_generator,\n",
    "    args=(unlabel_files, batch_size),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 96, 96, 3], [None])\n",
    ")\n",
    "\n",
    "label_train_dataset = tf.data.Dataset.from_generator(\n",
    "    label_data_generator,\n",
    "    args=(label_train_files, batch_size),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 96, 96, 3], [None])\n",
    ")\n",
    "\n",
    "label_test_dataset = tf.data.Dataset.from_generator(\n",
    "    label_data_generator,\n",
    "    args=(label_test_files, batch_size),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 96, 96, 3], [None])\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distorts the color distibutions of images\n",
    "class RandomColorAffine(layers.Layer):\n",
    "    def __init__(self, brightness=0, jitter=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.brightness = brightness\n",
    "        self.jitter = jitter\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"brightness\": self.brightness, \"jitter\": self.jitter})\n",
    "        return config\n",
    "\n",
    "    def call(self, images, training=True):\n",
    "        if training:\n",
    "            batch_size = tf.shape(images)[0]\n",
    "\n",
    "            # Same for all colors\n",
    "            brightness_scales = 1 + tf.random.uniform(\n",
    "                (batch_size, 1, 1, 1), minval=-self.brightness, maxval=self.brightness\n",
    "            )\n",
    "            # Different for all colors\n",
    "            jitter_matrices = tf.random.uniform(\n",
    "                (batch_size, 1, 3, 3), minval=-self.jitter, maxval=self.jitter\n",
    "            )\n",
    "\n",
    "            color_transforms = (\n",
    "                tf.eye(3, batch_shape=[batch_size, 1]) * brightness_scales\n",
    "                + jitter_matrices\n",
    "            )\n",
    "            images = tf.clip_by_value(tf.matmul(images, color_transforms), 0, 1)\n",
    "        return images\n",
    "\n",
    "# Image augmentation module\n",
    "def get_augmenter(min_area, brightness, jitter):\n",
    "    zoom_factor = 1.0 - math.sqrt(min_area)\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(image_size, image_size, image_channels)),\n",
    "            layers.Rescaling(1 / 255),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomTranslation(zoom_factor / 2, zoom_factor / 2),\n",
    "            layers.RandomZoom((-zoom_factor, 0.0), (-zoom_factor, 0.0)),\n",
    "            RandomColorAffine(brightness, jitter),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "def get_encoder():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(image_size, image_size, image_channels)),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(width, activation=\"relu\"),\n",
    "        ],\n",
    "        name=\"encoder\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 47, 47, 128)       3584      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 23, 23, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 11, 11, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 5, 5, 128)         147584    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3200)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               409728    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 856,064\n",
      "Trainable params: 856,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"projection_head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"linear_probe\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 516\n",
      "Trainable params: 516\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "(<tf.Tensor 'IteratorGetNext:0' shape=(None, 96, 96, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>)\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "(<tf.Tensor 'IteratorGetNext:0' shape=(None, 96, 96, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>)\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 07:22:42.802084: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n",
      "2022-11-06 07:22:43.535929: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 125s 4s/step - c_loss: 5.4678 - c_acc: 0.0154 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 2.3115 - val_p_acc: 0.2500\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 118s 4s/step - c_loss: 5.2624 - c_acc: 0.0152 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 1.3965 - val_p_acc: 0.2500\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 118s 4s/step - c_loss: 5.1478 - c_acc: 0.0213 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 1.4107 - val_p_acc: 0.2500\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 119s 4s/step - c_loss: 4.9700 - c_acc: 0.0355 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 1.4442 - val_p_acc: 0.2500\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 119s 4s/step - c_loss: 4.7136 - c_acc: 0.0450 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 1.3950 - val_p_acc: 0.2700\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 119s 4s/step - c_loss: 4.5809 - c_acc: 0.0531 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 1.3895 - val_p_acc: 0.2700\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 118s 4s/step - c_loss: 4.4552 - c_acc: 0.0609 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 1.3837 - val_p_acc: 0.2500\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 119s 4s/step - c_loss: 4.2717 - c_acc: 0.0755 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 1.3714 - val_p_acc: 0.2775\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 119s 4s/step - c_loss: 4.0893 - c_acc: 0.0888 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 1.3942 - val_p_acc: 0.2500\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 119s 4s/step - c_loss: 3.9067 - c_acc: 0.1144 - p_loss: 0.0000e+00 - p_acc: 0.0000e+00 - val_p_loss: 1.3805 - val_p_acc: 0.2550\n",
      "Maximal validation accuracy: 27.75%\n"
     ]
    }
   ],
   "source": [
    "# Define the contrastive model with model-subclassing\n",
    "class ContrastiveModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)\n",
    "        self.classification_augmenter = get_augmenter(**classification_augmentation)\n",
    "        self.encoder = get_encoder()\n",
    "        # Non-linear MLP as projection head\n",
    "        self.projection_head = keras.Sequential(\n",
    "            [\n",
    "                keras.Input(shape=(width,)),\n",
    "                layers.Dense(width, activation=\"relu\"),\n",
    "                layers.Dense(width),\n",
    "            ],\n",
    "            name=\"projection_head\",\n",
    "        )\n",
    "        # Single dense layer for linear probing\n",
    "        self.linear_probe = keras.Sequential(\n",
    "            [layers.Input(shape=(width,)), layers.Dense(4)], name=\"linear_probe\"\n",
    "        )\n",
    "\n",
    "        self.encoder.summary()\n",
    "        self.projection_head.summary()\n",
    "        self.linear_probe.summary()\n",
    "\n",
    "    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.contrastive_optimizer = contrastive_optimizer\n",
    "        self.probe_optimizer = probe_optimizer\n",
    "\n",
    "        # self.contrastive_loss will be defined as a method\n",
    "        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n",
    "        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
    "            name=\"c_acc\"\n",
    "        )\n",
    "        self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n",
    "        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.contrastive_loss_tracker,\n",
    "            self.contrastive_accuracy,\n",
    "            self.probe_loss_tracker,\n",
    "            self.probe_accuracy,\n",
    "        ]\n",
    "\n",
    "    def contrastive_loss(self, projections_1, projections_2):\n",
    "        # InfoNCE loss (information noise-contrastive estimation)\n",
    "        # NT-Xent loss (normalized temperature-scaled cross entropy)\n",
    "\n",
    "        # Cosine similarity: the dot product of the l2-normalized feature vectors\n",
    "        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n",
    "        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n",
    "        similarities = (\n",
    "            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature\n",
    "        )\n",
    "\n",
    "        # The similarity between the representations of two augmented views of the\n",
    "        # same image should be higher than their similarity with other views\n",
    "        batch_size = tf.shape(projections_1)[0]\n",
    "        contrastive_labels = tf.range(batch_size)\n",
    "        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n",
    "        self.contrastive_accuracy.update_state(\n",
    "            contrastive_labels, tf.transpose(similarities)\n",
    "        )\n",
    "\n",
    "        # The temperature-scaled similarities are used as logits for cross-entropy\n",
    "        # a symmetrized version of the loss is used here\n",
    "        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n",
    "            contrastive_labels, similarities, from_logits=True\n",
    "        )\n",
    "        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n",
    "            contrastive_labels, tf.transpose(similarities), from_logits=True\n",
    "        )\n",
    "        return (loss_1_2 + loss_2_1) / 2\n",
    "\n",
    "    def train_step(self, data):\n",
    "        print(data)\n",
    "        (unlabeled_images, _) = data\n",
    "\n",
    "        # Both labeled and unlabeled images are used, without labels\n",
    "        images = tf.concat((unlabeled_images), axis=0)\n",
    "        # Each image is augmented twice, differently\n",
    "        augmented_images_1 = self.contrastive_augmenter(images, training=True)\n",
    "        augmented_images_2 = self.contrastive_augmenter(images, training=True)\n",
    "        with tf.GradientTape() as tape:\n",
    "            features_1 = self.encoder(augmented_images_1, training=True)\n",
    "            features_2 = self.encoder(augmented_images_2, training=True)\n",
    "            # The representations are passed through a projection mlp\n",
    "            projections_1 = self.projection_head(features_1, training=True)\n",
    "            projections_2 = self.projection_head(features_2, training=True)\n",
    "            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n",
    "        gradients = tape.gradient(\n",
    "            contrastive_loss,\n",
    "            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
    "        )\n",
    "        self.contrastive_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                gradients,\n",
    "                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n",
    "            )\n",
    "        )\n",
    "        self.contrastive_loss_tracker.update_state(contrastive_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        labeled_images, labels = data\n",
    "\n",
    "        # For testing the components are used with a training=False flag\n",
    "        preprocessed_images = self.classification_augmenter(\n",
    "            labeled_images, training=False\n",
    "        )\n",
    "        features = self.encoder(preprocessed_images, training=False)\n",
    "        class_logits = self.linear_probe(features, training=False)\n",
    "        probe_loss = self.probe_loss(labels, class_logits)\n",
    "        self.probe_loss_tracker.update_state(probe_loss)\n",
    "        self.probe_accuracy.update_state(labels, class_logits)\n",
    "\n",
    "        # Only the probe metrics are logged at test time\n",
    "        return {m.name: m.result() for m in self.metrics[2:]}\n",
    "\n",
    "\n",
    "# Contrastive pretraining\n",
    "pretraining_model = ContrastiveModel()\n",
    "pretraining_model.compile(\n",
    "    contrastive_optimizer=keras.optimizers.Adam(),\n",
    "    probe_optimizer=keras.optimizers.Adam(),\n",
    ")\n",
    "\n",
    "pretraining_history = pretraining_model.fit(\n",
    "    unlabel_dataset, epochs=num_epochs, validation_data=label_train_dataset\n",
    ")\n",
    "print(\n",
    "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
    "        max(pretraining_history.history[\"val_p_acc\"]) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "2/2 [==============================] - 4s 825ms/step - loss: 1.6364 - acc: 0.3400 - val_loss: 1.3147 - val_acc: 0.2900\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 3s 739ms/step - loss: 1.2656 - acc: 0.3625 - val_loss: 1.3309 - val_acc: 0.2900\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 3s 718ms/step - loss: 1.2982 - acc: 0.2875 - val_loss: 1.2665 - val_acc: 0.3400\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 3s 734ms/step - loss: 1.2528 - acc: 0.3125 - val_loss: 1.1738 - val_acc: 0.4700\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 3s 721ms/step - loss: 1.1878 - acc: 0.4575 - val_loss: 1.0866 - val_acc: 0.5100\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 3s 745ms/step - loss: 1.1352 - acc: 0.5475 - val_loss: 1.0106 - val_acc: 0.6500\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 3s 743ms/step - loss: 1.0705 - acc: 0.5825 - val_loss: 0.9402 - val_acc: 0.6700\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 3s 718ms/step - loss: 1.0186 - acc: 0.5625 - val_loss: 0.8713 - val_acc: 0.7100\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 3s 744ms/step - loss: 0.9456 - acc: 0.6400 - val_loss: 0.8042 - val_acc: 0.6800\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 3s 729ms/step - loss: 0.8693 - acc: 0.7200 - val_loss: 0.7545 - val_acc: 0.6800\n",
      "Maximal validation accuracy: 71.00%\n"
     ]
    }
   ],
   "source": [
    "# Supervised finetuning of the pretrained encoder\n",
    "finetuning_model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(image_size, image_size, image_channels)),\n",
    "        get_augmenter(**classification_augmentation),\n",
    "        pretraining_model.encoder,\n",
    "        layers.Dense(4),\n",
    "    ],\n",
    "    name=\"finetuning_model\",\n",
    ")\n",
    "finetuning_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "finetuning_history = finetuning_model.fit(\n",
    "    label_train_dataset, epochs=num_epochs, validation_data=label_test_dataset\n",
    ")\n",
    "print(\n",
    "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
    "        max(finetuning_history.history[\"val_acc\"]) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "label_files = label_test_files.copy()\n",
    "label_files.extend(label_train_files)\n",
    "y_true_list = [int(y_true) for (data, y_true) in label_files]\n",
    "print(y_true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 45ms/step\n",
      "(500, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_dataset = tf.data.Dataset.from_generator(\n",
    "    label_data_generator,\n",
    "    args=(label_files, batch_size),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 96, 96, 3], [None])\n",
    ")\n",
    "emb = pretraining_model.encoder.predict(total_dataset)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 128)\n",
      "(500, 512)\n"
     ]
    }
   ],
   "source": [
    "padding_emb = emb.copy()\n",
    "print(padding_emb.shape)\n",
    "padding_result_emb = list()\n",
    "for i in padding_emb:\n",
    "    padding_result_emb.append(np.append(i, [0]*(512-128)))\n",
    "padding_result_emb = np.array(padding_result_emb)\n",
    "print(padding_result_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           0.          78.15087128   0.           0.\n",
      " 315.85351562  29.1821785   10.82477283   0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.         155.58505249   0.           0.\n",
      " 162.99966431   8.10432053   0.           0.          81.29200745\n",
      "   0.          54.78651047   0.           0.           0.\n",
      "   0.           0.          14.6925869   11.72392368   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "  50.51418686 219.35920715   0.           0.           0.\n",
      "   0.           0.         128.64035034   0.           0.\n",
      "   0.           0.           0.           0.          99.15350342\n",
      "   0.           0.         206.35559082   0.           0.\n",
      "   0.           0.         115.81131744   0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "  69.96138763   0.          64.55688477  55.65756226   0.\n",
      " 262.90252686   0.         104.70839691   0.           0.\n",
      " 164.01702881   0.           0.         293.24200439  60.24173355\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.          85.46797943 219.47485352  26.5845108    0.\n",
      " 265.10189819   0.         126.88904572   0.           0.\n",
      "   0.         223.5738678  335.39385986 276.07684326 287.22525024\n",
      "   0.           0.           0.           0.           0.\n",
      "  69.48973846   0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(padding_result_emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 512)\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9559999704360962"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def KNN(emb, cls, batch_size, Ks=[1, 10, 50, 100]):\n",
    "    \"\"\"Apply KNN for different K and return the maximum acc\"\"\"\n",
    "    preds = []\n",
    "    mask = torch.eye(batch_size).bool().to(emb.device)\n",
    "    mask = F.pad(mask, (0, len(emb) - batch_size))\n",
    "    for batch_x in torch.split(emb, batch_size):\n",
    "        dist = torch.norm(\n",
    "            batch_x.unsqueeze(1) - emb.unsqueeze(0), dim=2, p=\"fro\")\n",
    "        now_batch_size = len(batch_x)\n",
    "        mask = mask[:now_batch_size]\n",
    "        dist = torch.masked_fill(dist, mask, float('inf'))\n",
    "        # update mask\n",
    "        mask = F.pad(mask[:, :-now_batch_size], (now_batch_size, 0))\n",
    "        pred = []\n",
    "        for K in Ks:\n",
    "            knn = dist.topk(K, dim=1, largest=False).indices\n",
    "            knn = cls[knn].cpu()\n",
    "            pred.append(torch.mode(knn).values)\n",
    "        pred = torch.stack(pred, dim=0)\n",
    "        preds.append(pred)\n",
    "    preds = torch.cat(preds, dim=1)\n",
    "    accs = [(pred == cls.cpu()).float().mean().item() for pred in preds]\n",
    "    return max(accs)\n",
    "\n",
    "print(padding_result_emb.shape)\n",
    "print(y_true_list)\n",
    "KNN(torch.tensor(padding_result_emb), torch.tensor(np.array(y_true_list)), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./unlabeled/3362.jpg', './unlabeled/3370.jpg', './unlabeled/5102.jpg', './unlabeled/5279.jpg', './unlabeled/1144.jpg']\n",
      "['./unlabeled/0000.jpg', './unlabeled/0001.jpg', './unlabeled/0002.jpg', './unlabeled/0003.jpg', './unlabeled/0004.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(unlabel_files[:5])\n",
    "\n",
    "sort_unlabel_files = unlabel_files.copy()\n",
    "sort_unlabel_files = sorted(sort_unlabel_files)\n",
    "\n",
    "print(sort_unlabel_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./unlabeled/0000.jpg', './unlabeled/0001.jpg', './unlabeled/0002.jpg', './unlabeled/0003.jpg', './unlabeled/0004.jpg']\n",
      "29/29 [==============================] - 1s 25ms/step\n",
      "(7294, 128)\n"
     ]
    }
   ],
   "source": [
    "print(sort_unlabel_files[:5])\n",
    "# generate Dataset\n",
    "unlabel_test_dataset = tf.data.Dataset.from_generator(\n",
    "    unlabel_data_generator,\n",
    "    args=(sort_unlabel_files, batch_size),\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 96, 96, 3], [None])\n",
    ")\n",
    "unlabel_emb = pretraining_model.encoder.predict(unlabel_test_dataset)\n",
    "print(unlabel_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7294, 128)\n",
      "(7294, 512)\n"
     ]
    }
   ],
   "source": [
    "padding_unlabel_emb = unlabel_emb.copy()\n",
    "print(padding_unlabel_emb.shape)\n",
    "padding_result_unlabel_emb = list()\n",
    "for i in unlabel_emb:\n",
    "    padding_result_unlabel_emb.append(np.append(i, [0]*(512-128)))\n",
    "padding_result_unlabel_emb = np.array(padding_result_unlabel_emb)\n",
    "print(padding_result_unlabel_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.          0.          0.          0.          0.        224.3529\n",
      "  38.81175     0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        143.23363     0.\n",
      "   0.        147.00427     0.          0.          0.        116.69171\n",
      "  13.271267   78.16485     0.          0.          0.         72.33827\n",
      "   0.          0.         96.33503     0.          0.          0.\n",
      "   0.          0.          0.         54.97493   181.93388    45.70551\n",
      "   0.          0.          0.          0.        138.92345     0.\n",
      "   0.          0.          0.        105.34854     0.         80.96767\n",
      "   0.          0.        100.73903     0.          0.          0.\n",
      "   0.         83.009094    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        138.58371     0.          2.5910926  21.499939\n",
      "   0.        204.87994     0.        102.82436     0.          0.\n",
      " 200.06813     0.          0.        256.88623    30.83854     0.\n",
      "   0.          0.          0.          0.          0.        102.24079\n",
      " 161.31216   104.1697      0.         75.930435    0.         82.4449\n",
      "   0.          0.          0.        161.76636   234.47159   149.74976\n",
      " 175.33327     0.          0.          0.          0.          0.\n",
      " 102.067635    0.          0.          0.          0.          0.\n",
      "   0.          0.       ]\n",
      "[  0.           0.           0.           0.           0.\n",
      " 224.35290527  38.8117485    0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.         143.23362732   0.           0.\n",
      " 147.00427246   0.           0.           0.         116.69171143\n",
      "  13.27126694  78.16484833   0.           0.           0.\n",
      "  72.33827209   0.           0.          96.3350296    0.\n",
      "   0.           0.           0.           0.           0.\n",
      "  54.97492981 181.93388367  45.70550919   0.           0.\n",
      "   0.           0.         138.92344666   0.           0.\n",
      "   0.           0.         105.34854126   0.          80.96766663\n",
      "   0.           0.         100.73902893   0.           0.\n",
      "   0.           0.          83.00909424   0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      " 138.58370972   0.           2.59109259  21.49993896   0.\n",
      " 204.87994385   0.         102.82436371   0.           0.\n",
      " 200.06813049   0.           0.         256.88623047  30.83853912\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.         102.24079132 161.31216431 104.16970062   0.\n",
      "  75.93043518   0.          82.44490051   0.           0.\n",
      "   0.         161.76635742 234.47158813 149.74975586 175.33326721\n",
      "   0.           0.           0.           0.           0.\n",
      " 102.06763458   0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(unlabel_emb[0])\n",
    "print(padding_result_unlabel_emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"0711282\", padding_result_unlabel_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('3.8.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44883d22f80f18ef889a1dba7be5a0fad8fd72b0a76884ea66d1ab4eda69ec6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
